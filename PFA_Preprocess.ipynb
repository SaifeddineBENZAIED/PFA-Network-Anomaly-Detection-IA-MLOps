{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKi_2Ndj2N5V",
        "outputId": "afad74ac-d295-4db0-c47e-9c6d1dfd33ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Log setting\n",
        "logging.basicConfig(format=\"%(asctime)s %(levelname)s %(message)s\", datefmt=\"%H:%M:%S\", level=logging.INFO)\n",
        "\n",
        "# Change display.max_rows to show all features.\n",
        "pd.set_option(\"display.max_rows\", 85)"
      ],
      "metadata": {
        "id": "EHDSe_cvW6IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIR_PATH = \"/content/drive/MyDrive/PFA_codes/DataSets/CICIDS2017/MachineLearningCVE\"\n",
        "PROCESSED_DIR_PATH = \"/content/drive/MyDrive/PFA_codes/DataSets/CICIDS2017/ProcessedDataset\"\n",
        "FILE_PATH = os.path.join(DIR_PATH, \"MachineLearningCVE.csv\")"
      ],
      "metadata": {
        "id": "sAelLDuJW6SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _label_encoding() -> LabelEncoder:\n",
        "    # Create Label Encoder\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Read Label column from all dataset files\n",
        "    labels = pd.read_csv(FILE_PATH, usecols=['Label'], skipinitialspace=True)\n",
        "\n",
        "    # Fit the labels data to Label Encoder\n",
        "    le.fit(labels.Label)\n",
        "\n",
        "    # Saving the label encoder\n",
        "    np.save(os.path.join(PROCESSED_DIR_PATH, 'label_encoder.npy'), le.classes_)\n",
        "\n",
        "    # Log the result.\n",
        "    logging.info(\"Total rows: {}\".format(labels.shape))\n",
        "    logging.info(\"Class distribution:\\n{}\\n\".format(labels.Label.value_counts()))\n",
        "\n",
        "    return le\n",
        "\n",
        "\n",
        "def _process(df: pd.DataFrame, le: LabelEncoder) -> (np.ndarray, np.ndarray):\n",
        "    # Label encoding\n",
        "    df.Label = le.transform(df.Label)\n",
        "\n",
        "    # Fill NaN with average value of each class in this dataset\n",
        "    nan_rows = df[df.isna().any(axis=1)].shape[0]\n",
        "    logging.info(\"Fill NaN in {} rows with average value of each class.\".format(nan_rows))\n",
        "    df.iloc[:, df.columns != \"Label\"] = df.groupby(\"Label\").transform(lambda x: x.fillna(x.mean()))\n",
        "\n",
        "    # Change inf value with maximum value of each class\n",
        "    inf_rows = df[df.isin([np.inf]).any(axis=1)].shape[0]\n",
        "    logging.info(\"Replace Inf in {} rows with maximum value of each class.\".format(inf_rows))\n",
        "    # Temporary replace inf with NaN\n",
        "    df = df.replace([np.inf], np.nan)\n",
        "    # Replace inf with maximum value of each class in this dataset\n",
        "    df.iloc[:, df.columns != \"Label\"] = df.groupby(\"Label\").transform(lambda x: x.fillna(x.max()))\n",
        "\n",
        "    # Change negative value with minimum positive value of each class\n",
        "    logging.info(\"Replace negative values with minimum value of each class.\")\n",
        "    # Temporary replace negative value with NaN\n",
        "    df[df < 0] = np.nan\n",
        "    # Replace negative value with minimum value of each class in this dataset\n",
        "    df.iloc[:, df.columns != \"Label\"] = df.groupby(\"Label\").transform(lambda x: x.fillna(x.min()))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def _split_train_test(df: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n",
        "    # Sampling the dataset\n",
        "    x = df.drop(columns=['Label'])\n",
        "    y = df[['Label']]\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.20, random_state=np.random.randint(10))\n",
        "\n",
        "    del x, y\n",
        "\n",
        "    train = pd.concat([x_train, y_train], axis=1, sort=False)\n",
        "    test = pd.concat([x_test, y_test], axis=1, sort=False)\n",
        "\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def _to_csv(df: pd.DataFrame, saving_path: str):\n",
        "    # if file does not exist write header\n",
        "    if not os.path.isfile(saving_path):\n",
        "        df.to_csv(saving_path, index=False)\n",
        "    # else it exists so append without writing the header\n",
        "    else:\n",
        "        df.to_csv(saving_path, index=False, mode='a', header=False)\n",
        "\n",
        "\n",
        "def _preprocessing_all(le: LabelEncoder, chunksize=1000000):\n",
        "    # Preprocess all file\n",
        "    for chunk in pd.read_csv(FILE_PATH, skipinitialspace=True, chunksize=chunksize):\n",
        "        \"\"\"train, test = _split_train_test(_process(chunk, le))\n",
        "        _to_csv(train, os.path.join(PROCESSED_DIR_PATH, \"train_MachineLearningCVE.csv\"))\n",
        "        _to_csv(test, os.path.join(PROCESSED_DIR_PATH, \"test_MachineLearningCVE.csv\"))\"\"\"\n",
        "        # Process the chunk\n",
        "        processed_chunk = _process(chunk, le)\n",
        "\n",
        "        # Check if there is any class with only one instance\n",
        "        class_counts = processed_chunk['Label'].value_counts()\n",
        "        classes_with_single_instance = class_counts[class_counts == 1].index.tolist()\n",
        "\n",
        "        \"\"\"# Add another instance for each class with only one instance\n",
        "        for cls in classes_with_single_instance:\n",
        "            single_instance = processed_chunk[processed_chunk['Label'] == cls].iloc[0]\n",
        "            processed_chunk = processed_chunk.append(single_instance, ignore_index=True)\"\"\"\n",
        "\n",
        "        # Add another instance for each class with only one instance\n",
        "        for cls in classes_with_single_instance:\n",
        "            single_instance = processed_chunk[processed_chunk['Label'] == cls].iloc[0]\n",
        "            processed_chunk = pd.concat([processed_chunk, single_instance.to_frame().T], ignore_index=True)\n",
        "\n",
        "        # Split and save the processed chunk\n",
        "        train, test = _split_train_test(processed_chunk)\n",
        "        _to_csv(train, os.path.join(PROCESSED_DIR_PATH, \"train_MachineLearningCVE.csv\"))\n",
        "        _to_csv(test, os.path.join(PROCESSED_DIR_PATH, \"test_MachineLearningCVE.csv\"))"
      ],
      "metadata": {
        "id": "Im_gKL-NW6ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = _label_encoding()"
      ],
      "metadata": {
        "id": "Ee5-XHisW6zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"_preprocessing_all(label_encoder, 2500000)\"\"\"\n",
        "_preprocessing_all(label_encoder)"
      ],
      "metadata": {
        "id": "KTHcNsd6XMCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.info(\"*** END ***\")"
      ],
      "metadata": {
        "id": "v0iCG52MXMJV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}